{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/img/teclab_logo.png\" alt=\"Teclab logo\" width=\"170\">\n",
    "\n",
    "**Author**: Hector Vergara ([LinkedIn](https://www.linkedin.com/in/hector-vergara/))\n",
    "\n",
    "**Repository**: [nlp_apis](https://github.com/hhvergara/nlp_apis)\n",
    "\n",
    "**Python Notebook**: [API4.ipynb](https://github.com/hhvergara/nlp_apis/blob/main/API4.ipynb)\n",
    "\n",
    "----\n",
    "\n",
    "# API 4:\n",
    "\n",
    "### Contexto\n",
    "\n",
    "Una vez lograda la representaci√≥n vectorial del texto, se argumenta que ahora s√≠ se ha conseguido una data estructurada gracias al preprocesamiento de texto, y que este resultado, a su vez, puede ser INPUT para un modelo.\n",
    "\n",
    "Una vez m√°s, alguien del equipo expresa que quiere aplicar una regresi√≥n lineal, a lo que contestamos que no se puede porque el target de este problema no es num√©rico, y m√°s bien hay que ponerse a trabajar en un modelo supervisado para clasificaci√≥n.\n",
    "¬øQu√© modelos se aplicar√°n?\n",
    "\n",
    "### Consignas\n",
    "\n",
    "Modelo machine learning. Aplique un modelo machine learning -de los que Ud. ya conoce- para el problema de clasificaci√≥n.\n",
    "\n",
    "Se pueden utilizar los modelos de aprendizaje supervisado, tales como: random forest, support vector machine, vecinos m√°s cercanos (KNN), regresi√≥n log√≠stica, o Na√Øve Bayes. El modelo debe ajustarse con los vectores de la muestra de entrenamiento. Es importante que se considere que el target es multinomial y no binomial (sobre todo en la regresi√≥n log√≠stica).\n",
    "\n",
    "Evaluaci√≥n del modelo. Seg√∫n las predicciones de la muestra de testeo, realice la evaluaci√≥n del modelo. Para ello, calcule los √≠ndices de desempe√±o como acuracidad, recall y precisi√≥n. Interprete los resultados y exponga sus conclusiones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vinyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vinyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vinyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vinyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\vinyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vinyl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Library Imports\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from  nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "__version__ = '0.0.1'\n",
    "__email__ = 'hhvservice@gmail.com'\n",
    "__author__ = 'Hector Vergara'\n",
    "__annotations__ = 'https://www.linkedin.com/in/hector-vergara/'\n",
    "__base_dir__ = Path().absolute()\n",
    "__data_dir__ = os.path.join(__base_dir__, 'data')\n",
    "filename_data = os.path.join(__data_dir__, 'sentiment_analysis_dataset.csv')\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descargamos el dataset \"sentiment-analysis-dataset\" de kaggle para realizar las pruebas.\n",
    "\n",
    "Referencia: https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km¬≤)</th>\n",
       "      <th>Density (P/Km¬≤)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>night</td>\n",
       "      <td>70-100</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>97929</td>\n",
       "      <td>440.0</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>45195774</td>\n",
       "      <td>2736690.0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50e14c0bb8</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>2963243</td>\n",
       "      <td>28470.0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Australia</td>\n",
       "      <td>25499884</td>\n",
       "      <td>7682300.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>Wow... u just became cooler.</td>\n",
       "      <td>positive</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Austria</td>\n",
       "      <td>9006398</td>\n",
       "      <td>82400.0</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "5  28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
       "6  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "7  50e14c0bb8                                         Soooo high   \n",
       "8  e050245fbd                                        Both of you   \n",
       "9  fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n",
       "\n",
       "                                       selected_text sentiment Time of Tweet  \\\n",
       "0                I`d have responded, if I were going   neutral       morning   \n",
       "1                                           Sooo SAD  negative          noon   \n",
       "2                                        bullying me  negative         night   \n",
       "3                                     leave me alone  negative       morning   \n",
       "4                                      Sons of ****,  negative          noon   \n",
       "5  http://www.dothebouncy.com/smf - some shameles...   neutral         night   \n",
       "6                                                fun  positive       morning   \n",
       "7                                         Soooo high   neutral          noon   \n",
       "8                                        Both of you   neutral         night   \n",
       "9                       Wow... u just became cooler.  positive       morning   \n",
       "\n",
       "  Age of User              Country  Population -2020  Land Area (Km¬≤)  \\\n",
       "0        0-20          Afghanistan          38928346         652860.0   \n",
       "1       21-30              Albania           2877797          27400.0   \n",
       "2       31-45              Algeria          43851044        2381740.0   \n",
       "3       46-60              Andorra             77265            470.0   \n",
       "4       60-70               Angola          32866272        1246700.0   \n",
       "5      70-100  Antigua and Barbuda             97929            440.0   \n",
       "6        0-20            Argentina          45195774        2736690.0   \n",
       "7       21-30              Armenia           2963243          28470.0   \n",
       "8       31-45            Australia          25499884        7682300.0   \n",
       "9       46-60              Austria           9006398          82400.0   \n",
       "\n",
       "   Density (P/Km¬≤)  \n",
       "0               60  \n",
       "1              105  \n",
       "2               18  \n",
       "3              164  \n",
       "4               26  \n",
       "5              223  \n",
       "6               17  \n",
       "7              104  \n",
       "8                3  \n",
       "9              109  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(filename_data, sep=',', encoding='unicode_escape')\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cantidad de filas: 27480\n",
      "Cantidad de columnas: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "Cantidad de filas: {df.shape[0]}\n",
    "Cantidad de columnas: {df.shape[1]}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPPreprocessor:\n",
    "\n",
    "    tokenizer_pattern = (\n",
    "            r'[\\U0001F600-\\U0001F64F]'          # classic emojis\n",
    "            r'|[\\U0001F300-\\U0001F5FF]'         # nature, symbols\n",
    "            r'|[\\U0001F680-\\U0001F6FF]'         # transport\n",
    "            r'|[\\U0001F1E0-\\U0001F1FF]'         # Flags\n",
    "            r'|[\\U00002700-\\U000027BF]'         # various symbols\n",
    "            r'|[\\U0001F900-\\U0001F9FF]'         # gestures\n",
    "            r'|[\\U00002600-\\U000026FF]'         # ‚òÄ‚òÇ\n",
    "            r'|‚ù§|ü•∞'                            # specific emojis\n",
    "            r'|:\\)'                             # emoticon :)\n",
    "            r'|\\b\\w+\\b'                         # words (alphanumeric)\n",
    "        )\n",
    "\n",
    "    def __init__(self, text_column: str):\n",
    "        self.text_column = text_column\n",
    "\n",
    "    def clean_tokenize_text(self, text: str) -> list:\n",
    "        \"\"\" Tokenizes text and removes emojis, emoticons, and special characters.\"\"\"\n",
    "\n",
    "        tokenizer = RegexpTokenizer(self.tokenizer_pattern)\n",
    "        return tokenizer.tokenize(text.lower())\n",
    "\n",
    "    def _get_wordnet_pos_(self, treebank_tag) -> str:\n",
    "        \"\"\"\n",
    "        Converts nltk (Treebank) POS tags to WordNet tags.\n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN  # By default, use NOUN if no match found\n",
    "\n",
    "\n",
    "    def lemmatize_tokens(self, tokens: list) -> list:\n",
    "        \"\"\"Lemmatize tokens using POS tagging for greater accuracy.\"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        pos_tags = pos_tag(tokens)  # [('los', 'DT'), ('ni√±os', 'NNS'), ...]\n",
    "        return [\n",
    "            lemmatizer.lemmatize(token, self._get_wordnet_pos_(pos))\n",
    "            for token, pos in pos_tags\n",
    "        ]\n",
    "\n",
    "    def stem_tokens(self, tokens: list) -> list:\n",
    "        \"\"\"Stem tokens using PorterStemmer.\"\"\"\n",
    "        stemmer = PorterStemmer().stem\n",
    "        return [stemmer(token) for token in tokens]\n",
    "\n",
    "    def preprocess(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df['tokens'] = df[self.text_column].astype(str).apply(self.clean_tokenize_text)\n",
    "        df['lemmas'] = df['tokens'].apply(self.lemmatize_tokens)\n",
    "        df['stems'] = df['tokens'].apply(self.stem_tokens)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km¬≤)</th>\n",
       "      <th>Density (P/Km¬≤)</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "      <td>[i, d, have, responded, if, i, were, going]</td>\n",
       "      <td>[i, d, have, respond, if, i, be, go]</td>\n",
       "      <td>[i, d, have, respond, if, i, were, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105</td>\n",
       "      <td>[sooo, sad, i, will, miss, you, here, in, san,...</td>\n",
       "      <td>[sooo, sad, i, will, miss, you, here, in, san,...</td>\n",
       "      <td>[sooo, sad, i, will, miss, you, here, in, san,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18</td>\n",
       "      <td>[my, boss, is, bullying, me]</td>\n",
       "      <td>[my, bos, be, bully, me]</td>\n",
       "      <td>[my, boss, is, bulli, me]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164</td>\n",
       "      <td>[what, interview, leave, me, alone]</td>\n",
       "      <td>[what, interview, leave, me, alone]</td>\n",
       "      <td>[what, interview, leav, me, alon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26</td>\n",
       "      <td>[sons, of, why, couldn, t, they, put, them, on...</td>\n",
       "      <td>[son, of, why, couldn, t, they, put, them, on,...</td>\n",
       "      <td>[son, of, whi, couldn, t, they, put, them, on,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>night</td>\n",
       "      <td>70-100</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>97929</td>\n",
       "      <td>440.0</td>\n",
       "      <td>223</td>\n",
       "      <td>[http, www, dothebouncy, com, smf, some, shame...</td>\n",
       "      <td>[http, www, dothebouncy, com, smf, some, shame...</td>\n",
       "      <td>[http, www, dothebounci, com, smf, some, shame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>45195774</td>\n",
       "      <td>2736690.0</td>\n",
       "      <td>17</td>\n",
       "      <td>[2am, feedings, for, the, baby, are, fun, when...</td>\n",
       "      <td>[2am, feeding, for, the, baby, be, fun, when, ...</td>\n",
       "      <td>[2am, feed, for, the, babi, are, fun, when, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50e14c0bb8</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>2963243</td>\n",
       "      <td>28470.0</td>\n",
       "      <td>104</td>\n",
       "      <td>[soooo, high]</td>\n",
       "      <td>[soooo, high]</td>\n",
       "      <td>[soooo, high]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Australia</td>\n",
       "      <td>25499884</td>\n",
       "      <td>7682300.0</td>\n",
       "      <td>3</td>\n",
       "      <td>[both, of, you]</td>\n",
       "      <td>[both, of, you]</td>\n",
       "      <td>[both, of, you]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>Wow... u just became cooler.</td>\n",
       "      <td>positive</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Austria</td>\n",
       "      <td>9006398</td>\n",
       "      <td>82400.0</td>\n",
       "      <td>109</td>\n",
       "      <td>[journey, wow, u, just, became, cooler, hehe, ...</td>\n",
       "      <td>[journey, wow, u, just, become, cool, hehe, be...</td>\n",
       "      <td>[journey, wow, u, just, becam, cooler, hehe, i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "5  28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
       "6  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "7  50e14c0bb8                                         Soooo high   \n",
       "8  e050245fbd                                        Both of you   \n",
       "9  fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n",
       "\n",
       "                                       selected_text sentiment Time of Tweet  \\\n",
       "0                I`d have responded, if I were going   neutral       morning   \n",
       "1                                           Sooo SAD  negative          noon   \n",
       "2                                        bullying me  negative         night   \n",
       "3                                     leave me alone  negative       morning   \n",
       "4                                      Sons of ****,  negative          noon   \n",
       "5  http://www.dothebouncy.com/smf - some shameles...   neutral         night   \n",
       "6                                                fun  positive       morning   \n",
       "7                                         Soooo high   neutral          noon   \n",
       "8                                        Both of you   neutral         night   \n",
       "9                       Wow... u just became cooler.  positive       morning   \n",
       "\n",
       "  Age of User              Country  Population -2020  Land Area (Km¬≤)  \\\n",
       "0        0-20          Afghanistan          38928346         652860.0   \n",
       "1       21-30              Albania           2877797          27400.0   \n",
       "2       31-45              Algeria          43851044        2381740.0   \n",
       "3       46-60              Andorra             77265            470.0   \n",
       "4       60-70               Angola          32866272        1246700.0   \n",
       "5      70-100  Antigua and Barbuda             97929            440.0   \n",
       "6        0-20            Argentina          45195774        2736690.0   \n",
       "7       21-30              Armenia           2963243          28470.0   \n",
       "8       31-45            Australia          25499884        7682300.0   \n",
       "9       46-60              Austria           9006398          82400.0   \n",
       "\n",
       "   Density (P/Km¬≤)                                             tokens  \\\n",
       "0               60        [i, d, have, responded, if, i, were, going]   \n",
       "1              105  [sooo, sad, i, will, miss, you, here, in, san,...   \n",
       "2               18                       [my, boss, is, bullying, me]   \n",
       "3              164                [what, interview, leave, me, alone]   \n",
       "4               26  [sons, of, why, couldn, t, they, put, them, on...   \n",
       "5              223  [http, www, dothebouncy, com, smf, some, shame...   \n",
       "6               17  [2am, feedings, for, the, baby, are, fun, when...   \n",
       "7              104                                      [soooo, high]   \n",
       "8                3                                    [both, of, you]   \n",
       "9              109  [journey, wow, u, just, became, cooler, hehe, ...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0               [i, d, have, respond, if, i, be, go]   \n",
       "1  [sooo, sad, i, will, miss, you, here, in, san,...   \n",
       "2                           [my, bos, be, bully, me]   \n",
       "3                [what, interview, leave, me, alone]   \n",
       "4  [son, of, why, couldn, t, they, put, them, on,...   \n",
       "5  [http, www, dothebouncy, com, smf, some, shame...   \n",
       "6  [2am, feeding, for, the, baby, be, fun, when, ...   \n",
       "7                                      [soooo, high]   \n",
       "8                                    [both, of, you]   \n",
       "9  [journey, wow, u, just, become, cool, hehe, be...   \n",
       "\n",
       "                                               stems  \n",
       "0             [i, d, have, respond, if, i, were, go]  \n",
       "1  [sooo, sad, i, will, miss, you, here, in, san,...  \n",
       "2                          [my, boss, is, bulli, me]  \n",
       "3                  [what, interview, leav, me, alon]  \n",
       "4  [son, of, whi, couldn, t, they, put, them, on,...  \n",
       "5  [http, www, dothebounci, com, smf, some, shame...  \n",
       "6  [2am, feed, for, the, babi, are, fun, when, he...  \n",
       "7                                      [soooo, high]  \n",
       "8                                    [both, of, you]  \n",
       "9  [journey, wow, u, just, becam, cooler, hehe, i...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage:\n",
    "preprocessor = NLPPreprocessor(text_column='text')\n",
    "processed_df = preprocessor.preprocess(df)\n",
    "processed_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de filas en train: 21984\n",
      "Cantidad de columnas en train: 13\n",
      "Cantidad de filas en test: 5496\n",
      "Cantidad de columnas en test: 13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(processed_df, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = train_df['text']\n",
    "x_test = test_df['text']\n",
    "y_train = train_df['sentiment']\n",
    "y_test = test_df['sentiment']\n",
    "\n",
    "print(f'''Cantidad de filas en train: {train_df.shape[0]}\n",
    "Cantidad de columnas en train: {train_df.shape[1]}\n",
    "Cantidad de filas en test: {test_df.shape[0]}\n",
    "Cantidad de columnas en test: {test_df.shape[1]}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_x_train shape: (21984, 185308)\n",
      "tfidf_x_test shape: (5496, 185308)\n"
     ]
    }
   ],
   "source": [
    "# Model creation using TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(token_pattern=r'[^\\s]+', ngram_range=(1, 2), max_features=30000)\n",
    "\n",
    "# Vectorize the text data\n",
    "tfidf_x_train = tfidf_vectorizer.fit_transform(x_train)\n",
    "tfidf_x_test = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "print(f'''tfidf_x_train shape: {tfidf_x_train.shape}\\ntfidf_x_test shape: {tfidf_x_test.shape}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.59 GiB for an array with shape (5496, 185308) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert the TF-IDF matrix back to a DataFrame\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tfidf_x_train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tfidf_x_train\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mfeature_names)\n\u001b[1;32m----> 6\u001b[0m tfidf_x_test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mtfidf_x_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39mfeature_names)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mtfidf_x_train_df shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtfidf_x_train_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtfidf_x_test_df shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtfidf_x_test_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'''\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vinyl\\OneDrive\\Documentos\\Teclab\\nlp_apis\\.venv\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1181\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1180\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1181\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vinyl\\OneDrive\\Documentos\\Teclab\\nlp_apis\\.venv\\Lib\\site-packages\\scipy\\sparse\\_base.py:1301\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 7.59 GiB for an array with shape (5496, 185308) and data type float64"
     ]
    }
   ],
   "source": [
    "# Get feature names from the TF-IDF vectorizer\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix back to a DataFrame\n",
    "tfidf_x_train_df = pd.DataFrame(tfidf_x_train.toarray(), columns=feature_names)\n",
    "tfidf_x_test_df = pd.DataFrame(tfidf_x_test.toarray(), columns=feature_names)\n",
    "\n",
    "print(f'''tfidf_x_train_df shape: {tfidf_x_train_df.shape}\\ntfidf_x_test_df shape: {tfidf_x_test_df.shape}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of the TF-IDF DataFrame\n",
    "tfidf_x_train_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of the TF-IDF test DataFrame\n",
    "tfidf_x_test_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## ‚úÖ Entrenamiento del modelo: Random Forest Classifier\n",
    "# Entrenamos un modelo supervisado de clasificaci√≥n multiclase con los vectores TF-IDF.\n",
    "\n",
    "# %%\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(tfidf_x_train, y_train)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = clf.predict(tfidf_x_test)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"üìä Resultados de evaluaci√≥n:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\\n\")\n",
    "\n",
    "# Reporte completo\n",
    "print(\"üìã Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## ‚úÖ Modelo alternativo: Naive Bayes (MultinomialNB)\n",
    "# Probaremos ahora con un modelo de Naive Bayes, que suele tener buen rendimiento con TF-IDF en clasificaci√≥n de texto.\n",
    "\n",
    "# %%\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Entrenamiento\n",
    "nb = MultinomialNB()\n",
    "nb.fit(tfidf_x_train, y_train)\n",
    "\n",
    "# Predicci√≥n\n",
    "y_pred_nb = nb.predict(tfidf_x_test)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "accuracy = accuracy_score(y_test, y_pred_nb)\n",
    "precision = precision_score(y_test, y_pred_nb, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred_nb, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"üìä Resultados Naive Bayes:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\\n\")\n",
    "\n",
    "# Reporte completo\n",
    "print(\"üìã Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_nb, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "# ## ‚úÖ Modelo alternativo: Regresi√≥n Log√≠stica Multinomial\n",
    "# Utilizamos un modelo lineal eficiente que soporta clasificaci√≥n multiclase.\n",
    "\n",
    "# %%\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Entrenar el modelo\n",
    "lr = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
    "lr.fit(tfidf_x_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_lr = lr.predict(tfidf_x_test)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "precision = precision_score(y_test, y_pred_lr, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred_lr, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"üìä Resultados Logistic Regression:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\\n\")\n",
    "\n",
    "# Reporte completo\n",
    "print(\"üìã Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definimos el espacio de b√∫squeda\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # fuerza de regularizaci√≥n\n",
    "    'penalty': ['l2'],             # s√≥lo 'l2' funciona con solver 'lbfgs' y multiclase\n",
    "    'solver': ['lbfgs'],\n",
    "    'multi_class': ['multinomial']\n",
    "}\n",
    "\n",
    "# Creamos el modelo base\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# GridSearch con validaci√≥n cruzada (cv=3)\n",
    "grid_search = GridSearchCV(log_reg, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(tfidf_x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Mejores hiperpar√°metros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Mejor accuracy promedio en CV:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(tfidf_x_test)\n",
    "\n",
    "print(\"üìä Resultados fine-tuned Logistic Regression:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_best, average='weighted', zero_division=0))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_best, average='weighted', zero_division=0))\n",
    "print(\"\\nüìã Classification Report:\\n\", classification_report(y_test, y_pred_best, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "\n",
    "Aplicamos el modelo para generar la representaci√≥n vectorial, usando TfidfVectorizer e incluyendo el \n",
    "\n",
    "```\n",
    "token_pattern=r'[^\\s]+'\n",
    "```\n",
    "\n",
    "Por otra parte, realizamos el ajuste y transformaci√≥n de los datos X de test, y por ultimo mostramos el DF resultante para cada caso.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_apis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
